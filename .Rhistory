# Construct the confusion matrix (2x2) for McNemar's test
table <- matrix(c(confusion1["TP"], confusion1["FP"], confusion1["FN"], confusion1["TN"]), nrow = 2)
# Run McNemar's test
test_result <- mcnemar.test(table)
return(tibble(Model1 = model1, Model2 = model2, p_value = test_result$p.value))
} else {
cat("Skipping", model1, "vs", model2, "due to missing data.\n")
return(NULL)
}
}
# Get the unique models
models <- unique(long_perf$Model)
# Initialize an empty list to store the results
results <- list()
# Loop through all pairs of models and run McNemar's test
for (i in 1:(length(models) - 1)) {
for (j in (i + 1):length(models)) {
model1 <- models[i]
model2 <- models[j]
# Run the test and save the results
test_result <- run_mcnemar_test(long_perf, model1, model2)
if (!is.null(test_result)) {
results[[length(results) + 1]] <- test_result
}
}
}
# Combine all results into a data frame
comparison_results <- bind_rows(results)
# View the comparison results
print(comparison_results)
View(comparison_results)
write.csv(comparison_results, "McNemar test results.csv")
# Assuming long_perf is your data frame with the structure: Metric, Model, Value
# Define the output list to store test results
test_results <- list()
# Loop through the unique metrics
for (metric in unique(long_perf$Metric)) {
# Subset the data for the current metric
metric_data <- long_perf %>% filter(Metric == metric)
# Get the unique models
models <- unique(metric_data$Model)
# Loop through each combination of models
for (i in 1:(length(models)-1)) {
for (j in (i+1):length(models)) {
# Extract the data for the two models
model1 <- models[i]
model2 <- models[j]
# Extract the values for the metric and models
values_model1 <- metric_data %>% filter(Model == model1) %>% pull(Value)
values_model2 <- metric_data %>% filter(Model == model2) %>% pull(Value)
# Perform the t-test if the data is normally distributed
# You can use a check for normality first (e.g., Shapiro-Wilk test)
normal_test1 <- shapiro.test(values_model1)
normal_test2 <- shapiro.test(values_model2)
if (normal_test1$p.value > 0.05 && normal_test2$p.value > 0.05) {
# If both models pass normality, use t-test
test_result <- t.test(values_model1, values_model2, paired = TRUE)
test_type <- "t-test"
} else {
# Otherwise, use the Wilcoxon signed-rank test
test_result <- wilcox.test(values_model1, values_model2, paired = TRUE)
test_type <- "Wilcoxon"
}
# Store the result in the test_results list
test_results[[paste(metric, model1, model2, sep = "_")]] <- list(
Metric = metric,
Model1 = model1,
Model2 = model2,
Test_Type = test_type,
P_Value = test_result$p.value,
Test_Statistic = test_result$statistic
)
}
}
}
# Assuming long_perf is your data frame with the structure: Metric, Model, Value
# Define the output list to store test results
test_results <- list()
# Loop through the unique metrics
for (metric in unique(long_perf$Metric)) {
# Subset the data for the current metric
metric_data <- long_perf %>% filter(Metric == metric)
# Get the unique models
models <- unique(metric_data$Model)
# Loop through each combination of models
for (i in 1:(length(models)-1)) {
for (j in (i+1):length(models)) {
# Extract the data for the two models
model1 <- models[i]
model2 <- models[j]
# Extract the values for the metric and models
values_model1 <- metric_data %>% filter(Model == model1) %>% pull(Value)
values_model2 <- metric_data %>% filter(Model == model2) %>% pull(Value)
# Check if the values for either model are identical (no variance)
if (length(unique(values_model1)) == 1 | length(unique(values_model2)) == 1) {
test_result <- list(p.value = NA, statistic = NA, test = "Invalid (identical values)")
} else {
# Perform the t-test if the data is normally distributed
normal_test1 <- shapiro.test(values_model1)
normal_test2 <- shapiro.test(values_model2)
if (normal_test1$p.value > 0.05 && normal_test2$p.value > 0.05) {
# If both models pass normality, use t-test
test_result <- t.test(values_model1, values_model2, paired = TRUE)
test_type <- "t-test"
} else {
# Otherwise, use the Wilcoxon signed-rank test
test_result <- wilcox.test(values_model1, values_model2, paired = TRUE)
test_type <- "Wilcoxon"
}
}
# Store the result in the test_results list
test_results[[paste(metric, model1, model2, sep = "_")]] <- list(
Metric = metric,
Model1 = model1,
Model2 = model2,
Test_Type = test_type,
P_Value = test_result$p.value,
Test_Statistic = test_result$statistic
)
}
}
}
# Define the metrics to run the t-test or Wilcoxon test
metrics_to_test <- c("Precision", "F1", "RMSE")
# Create an empty list to store the results
test_results <- list()
# Loop through each metric
for (metric in metrics_to_test) {
# Filter out the data for the current metric
values_model1 <- long_perf %>% filter(Metric == metric & Model == model1) %>% pull(Value)
values_model2 <- long_perf %>% filter(Metric == metric & Model == model2) %>% pull(Value)
# Skip this metric if there are missing values
if (length(values_model1) < 3 | length(values_model2) < 3) next
# Check if the distribution is normal
if (shapiro.test(values_model1)$p.value > 0.05 & shapiro.test(values_model2)$p.value > 0.05) {
# If normal, use t-test
test_result <- t.test(values_model1, values_model2, paired = TRUE)
} else {
# Otherwise use Wilcoxon signed-rank test
test_result <- wilcox.test(values_model1, values_model2, paired = TRUE)
}
# Store the test result in the list
test_results[[metric]] <- test_result
}
# Print the test results
test_results
View(test_results)
# Get all unique models from the 'long_perf' data
models <- unique(long_perf$Model)
# Define the metrics to run the t-test or Wilcoxon test
metrics_to_test <- c("Precision", "F1", "RMSE")
# Create an empty list to store the results
test_results <- list()
# Loop through each pair of models
for (model1 in models) {
for (model2 in models) {
# Skip if comparing the model to itself
if (model1 == model2) next
# Loop through each metric
for (metric in metrics_to_test) {
# Filter out the data for the current metric and models
values_model1 <- long_perf %>% filter(Metric == metric & Model == model1) %>% pull(Value)
values_model2 <- long_perf %>% filter(Metric == metric & Model == model2) %>% pull(Value)
# Skip this metric if there are missing values or too few data points
if (length(values_model1) < 3 | length(values_model2) < 3) next
# Check if the distribution is normal
if (shapiro.test(values_model1)$p.value > 0.05 & shapiro.test(values_model2)$p.value > 0.05) {
# If normal, use t-test
test_result <- t.test(values_model1, values_model2, paired = TRUE)
} else {
# Otherwise use Wilcoxon signed-rank test
test_result <- wilcox.test(values_model1, values_model2, paired = TRUE)
}
# Store the test result in the list, along with the models and metric
test_results[[paste(model1, model2, metric, sep = "_")]] <- test_result
}
}
}
# Print the test results
test_results
test_results[["Naive.Bayes_Logistic_Precision"]]
# Create an empty data frame to store the results
pvalue_results <- data.frame(Model1 = character(),
Model2 = character(),
Metric = character(),
PValue = numeric(),
stringsAsFactors = FALSE)
# Loop through the test results and extract the p-values
for (result_name in names(test_results)) {
test_result <- test_results[[result_name]]
# Extract the model names and metric from the result name
result_parts <- strsplit(result_name, "_")[[1]]
model1 <- result_parts[1]
model2 <- result_parts[2]
metric <- result_parts[3]
# Extract the p-value from the test result
pvalue <- test_result$p.value
# Add the extracted values to the data frame
pvalue_results <- rbind(pvalue_results, data.frame(Model1 = model1,
Model2 = model2,
Metric = metric,
PValue = pvalue))
}
# View the final p-value table
print(pvalue_results)
View(pvalue_results)
# Create an empty data frame to store the results
pvalue_results <- data.frame(Model1 = character(),
Model2 = character(),
Metric = character(),
PValue = numeric(),
stringsAsFactors = FALSE)
# Loop through the test results and extract the p-values
for (result_name in names(test_results)) {
test_result <- test_results[[result_name]]
# Extract the model names and metric from the result name
result_parts <- strsplit(result_name, "_")[[1]]
model1 <- result_parts[1]
model2 <- result_parts[2]
metric <- result_parts[3]
# Ensure we don't add reverse pairs (e.g., "lasso vs logistic" and "logistic vs lasso")
model_pair <- sort(c(model1, model2))  # Sort models alphabetically
# Create a unique pair identifier
pair_id <- paste(model_pair, collapse = "_")
# Check if the pair already exists in the results
if (!(pair_id %in% pvalue_results$PairID)) {
# Extract the p-value from the test result
pvalue <- test_result$p.value
# Add the extracted values to the data frame, including the unique PairID
pvalue_results <- rbind(pvalue_results, data.frame(Model1 = model1,
Model2 = model2,
Metric = metric,
PValue = pvalue,
PairID = pair_id))
}
}
# Remove the PairID column for final output
pvalue_results <- pvalue_results[, -which(names(pvalue_results) == "PairID")]
# View the final p-value table
print(pvalue_results)
# Create an empty data frame to store the results for accuracy metrics
accuracy_pvalue_results <- data.frame(Model1 = character(),
Model2 = character(),
Metric = character(),
PValue = numeric(),
stringsAsFactors = FALSE)
# Loop through the test results and extract the p-values for accuracy metrics
for (result_name in names(test_results)) {
test_result <- test_results[[result_name]]
# Extract the model names and metric from the result name
result_parts <- strsplit(result_name, "_")[[1]]
model1 <- result_parts[1]
model2 <- result_parts[2]
metric <- result_parts[3]
# Only process accuracy metrics
if (metric == "accuracy") {
# Ensure we don't add reverse pairs (e.g., "lasso vs logistic" and "logistic vs lasso")
model_pair <- sort(c(model1, model2))  # Sort models alphabetically
# Create a unique pair identifier
pair_id <- paste(model_pair, collapse = "_")
# Check if the pair already exists in the results
if (!(pair_id %in% accuracy_pvalue_results$PairID)) {
# Extract the p-value from the test result
pvalue <- test_result$p.value
# Add the extracted values to the data frame, including the unique PairID
accuracy_pvalue_results <- rbind(accuracy_pvalue_results, data.frame(Model1 = model1,
Model2 = model2,
Metric = metric,
PValue = pvalue,
PairID = pair_id))
}
}
}
# Remove the PairID column for final output
accuracy_pvalue_results <- accuracy_pvalue_results[, -which(names(accuracy_pvalue_results) == "PairID")]
# View the final p-value table for accuracy
print(accuracy_pvalue_results)
# Create an empty data frame to store the results for accuracy metrics
accuracy_pvalue_results <- data.frame(Model1 = character(),
Model2 = character(),
Metric = character(),
PValue = numeric(),
stringsAsFactors = FALSE)
# Loop through the test results and extract the p-values for accuracy metrics
for (result_name in names(test_results)) {
test_result <- test_results[[result_name]]
# Extract the model names and metric from the result name
result_parts <- strsplit(result_name, "_")[[1]]
model1 <- result_parts[1]
model2 <- result_parts[2]
metric <- result_parts[3]
# Only process accuracy metrics
if (metric == "Accuracy") {
# Ensure we don't add reverse pairs (e.g., "lasso vs logistic" and "logistic vs lasso")
model_pair <- sort(c(model1, model2))  # Sort models alphabetically
# Create a unique pair identifier
pair_id <- paste(model_pair, collapse = "_")
# Check if the pair already exists in the results
if (!(pair_id %in% accuracy_pvalue_results$PairID)) {
# Extract the p-value from the test result
pvalue <- test_result$p.value
# Add the extracted values to the data frame, including the unique PairID
accuracy_pvalue_results <- rbind(accuracy_pvalue_results, data.frame(Model1 = model1,
Model2 = model2,
Metric = metric,
PValue = pvalue,
PairID = pair_id))
}
}
}
# Remove the PairID column for final output
accuracy_pvalue_results <- accuracy_pvalue_results[, -which(names(accuracy_pvalue_results) == "PairID")]
# View the final p-value table for accuracy
print(accuracy_pvalue_results)
# Create an empty data frame to store the results for accuracy metrics
accuracy_pvalue_results <- data.frame(Model1 = character(),
Model2 = character(),
Metric = character(),
PValue = numeric(),
stringsAsFactors = FALSE)
# Loop through the test results and extract the p-values for accuracy metrics
for (result_name in names(test_results)) {
test_result <- test_results[[result_name]]
# Extract the model names and metric from the result name
result_parts <- strsplit(result_name, "_")[[1]]
model1 <- result_parts[1]
model2 <- result_parts[2]
metric <- result_parts[3]
# Debugging print to check the metrics
print(paste("Processing:", result_name, "Metric:", metric))
# Only process accuracy metrics
if (metric == "accuracy") {
# Ensure we don't add reverse pairs (e.g., "lasso vs logistic" and "logistic vs lasso")
model_pair <- sort(c(model1, model2))  # Sort models alphabetically
# Create a unique pair identifier
pair_id <- paste(model_pair, collapse = "_")
# Check if the pair already exists in the results
if (!(pair_id %in% accuracy_pvalue_results$PairID)) {
# Extract the p-value from the test result
pvalue <- test_result$p.value
# Add the extracted values to the data frame, including the unique PairID
accuracy_pvalue_results <- rbind(accuracy_pvalue_results, data.frame(Model1 = model1,
Model2 = model2,
Metric = metric,
PValue = pvalue,
PairID = pair_id))
}
}
}
# Remove the PairID column for final output
accuracy_pvalue_results <- accuracy_pvalue_results[, -which(names(accuracy_pvalue_results) == "PairID")]
# View the final p-value table for accuracy
print(accuracy_pvalue_results)
# Create an empty data frame to store the results for accuracy metrics
accuracy_pvalue_results <- data.frame(Model1 = character(),
Model2 = character(),
Metric = character(),
PValue = numeric(),
stringsAsFactors = FALSE)
# Loop through the test results and extract the p-values for accuracy metrics
for (result_name in names(test_results)) {
test_result <- test_results[[result_name]]
# Extract the model names and metric from the result name
result_parts <- strsplit(result_name, "_")[[1]]
model1 <- result_parts[1]
model2 <- result_parts[2]
metric <- result_parts[3]
# Only process accuracy metrics
if (metric == "F1") {
# Ensure we don't add reverse pairs (e.g., "lasso vs logistic" and "logistic vs lasso")
model_pair <- sort(c(model1, model2))  # Sort models alphabetically
# Create a unique pair identifier
pair_id <- paste(model_pair, collapse = "_")
# Check if the pair already exists in the results
if (!(pair_id %in% accuracy_pvalue_results$PairID)) {
# Extract the p-value from the test result
pvalue <- test_result$p.value
# Add the extracted values to the data frame, including the unique PairID
accuracy_pvalue_results <- rbind(accuracy_pvalue_results, data.frame(Model1 = model1,
Model2 = model2,
Metric = metric,
PValue = pvalue,
PairID = pair_id))
}
}
}
# Remove the PairID column for final output
accuracy_pvalue_results <- accuracy_pvalue_results[, -which(names(accuracy_pvalue_results) == "PairID")]
# View the final p-value table for accuracy
print(accuracy_pvalue_results)
# Assuming 'summary_table' has the F1 scores for each model and 'accuracy_pvalue_results' contains the p-values for comparisons
# Join the significant p-values (you might want to filter out non-significant p-values here)
significant_pvalues <- accuracy_pvalue_results %>%
filter(p_value < 0.05) %>%
select(Model1, Model2, p_value)  # Adjust according to the structure of your results
# Assuming 'summary_table' has the F1 scores for each model and 'accuracy_pvalue_results' contains the p-values for comparisons
# Join the significant p-values (you might want to filter out non-significant p-values here)
significant_pvalues <- accuracy_pvalue_results %>%
filter(PValue < 0.05) %>%
select(Model1, Model2, PValue)  # Adjust according to the structure of your results
# Prepare the F1 data (make sure 'summary_table' has Model and Metric columns)
f1_data <- summary_table %>%
filter(Metric == "F1")
# Join the significant p-values with the F1 data (you can use Model1 or Model2 depending on which model you're comparing)
f1_with_pvalues <- f1_data %>%
left_join(significant_pvalues, by = c("Model" = "Model1")) %>%
mutate(p_value_label = ifelse(!is.na(PValue), paste("p =", round(PValue, 3)), NA))
# Plot the F1 bar graph with the significant p-values
ggplot(f1_with_pvalues, aes(x = Model, y = Mean, fill = Model)) +
geom_bar(stat = "identity", position = "dodge") +
scale_fill_manual(values = colors) +
facet_wrap(~ Metric, scales = "free_y") +  # Each metric in its own panel
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none"
) +
labs(
title = "Performance Metrics by Model",
x = "Model",
y = "Mean"
) +
geom_text(aes(label = p_value_label, y = Mean + 0.05), size = 3, color = "black")  # Adjust the y position to avoid overlapping
# Join the significant p-values (you might want to filter out non-significant p-values here)
significant_pvalues <- accuracy_pvalue_results %>%
filter(PValue < 0.05) %>%
select(Model1, Model2, PValue)  # Adjust according to the structure of your results
# Prepare the F1 data (make sure 'summary_table' has Model and Metric columns)
f1_data <- summary_table %>%
filter(Metric == "F1")
# Join the significant p-values with the F1 data (you can use Model1 or Model2 depending on which model you're comparing)
f1_with_pvalues <- f1_data %>%
left_join(significant_pvalues, by = c("Model" = "Model1")) %>%
mutate(p_value_label = ifelse(!is.na(PValue), paste("p =", round(PValue, 3)), NA)) %>%
mutate(Model = factor(Model, levels = model_order))  # Set the model order
# Plot the F1 bar graph with the significant p-values and error bars
ggplot(f1_with_pvalues, aes(x = Model, y = Mean, fill = Model)) +
geom_bar(stat = "identity", position = "dodge", width = 0.7) +  # Adjust width for better visibility
scale_fill_manual(values = colors) +  # Apply the custom color palette
facet_wrap(~ Metric, scales = "free_y") +  # Each metric in its own panel
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none"
) +
labs(
title = "Performance Metrics by Model",
x = "Model",
y = "Mean"
) +
geom_text(aes(label = p_value_label, y = Mean + 0.05), size = 3, color = "black") +  # Add p-values to bars
geom_errorbar(aes(ymin = Mean - SE, ymax = Mean + SE), width = 0.2)  # Add error bars (adjust SE as needed)
# Join the significant p-values (you might want to filter out non-significant p-values here)
significant_pvalues <- accuracy_pvalue_results %>%
filter(PValue < 0.05) %>%
select(Model1, Model2, PValue)  # Adjust according to the structure of your results
# Prepare the F1 data (make sure 'summary_table' has Model and Metric columns)
f1_data <- summary_table %>%
filter(Metric == "F1")
# Join the significant p-values with the F1 data (you can use Model1 or Model2 depending on which model you're comparing)
f1_with_pvalues <- f1_data %>%
left_join(significant_pvalues, by = c("Model" = "Model1")) %>%
mutate(p_value_label = ifelse(!is.na(PValue), paste("p =", round(PValue, 3)), NA)) %>%
mutate(Model = factor(Model, levels = model_order))  # Set the model order
# Plot the F1 bar graph with the significant p-values and error bars
ggplot(f1_with_pvalues, aes(x = Model, y = Mean, fill = Model)) +
geom_bar(stat = "identity", position = "dodge", width = 0.7) +  # Adjust width for better visibility
scale_fill_manual(values = colors) +  # Apply the custom color palette
facet_wrap(~ Metric, scales = "free_y") +  # Each metric in its own panel
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none"
) +
labs(
title = "Performance Metrics by Model",
x = "Model",
y = "Mean"
) +
geom_text(aes(label = p_value_label, y = Mean + 0.05), size = 3, color = "black") +  # Add p-values to bars
geom_errorbar(aes(ymin = Mean - SEM, ymax = Mean + SEM), width = 0.2)  # Add error bars (adjust SE as needed)
# Add a label like "Model1 vs Model2\np = 0.012"
f1_with_pvalues <- f1_with_pvalues %>%
mutate(p_value_label = ifelse(!is.na(PValue),
paste(Model, "vs", Model2, "\np =", round(PValue, 3)),
NA))
# Plot with better label position
ggplot(f1_with_pvalues, aes(x = Model, y = Mean, fill = Model)) +
geom_bar(stat = "identity", position = "dodge", width = 0.7) +
scale_fill_manual(values = colors) +
facet_wrap(~ Metric, scales = "free_y") +
theme_minimal() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none"
) +
labs(
title = "Performance Metrics by Model",
x = "Model",
y = "Mean"
) +
geom_errorbar(aes(ymin = Mean - SEM, ymax = Mean + SEM), width = 0.2) +
geom_text(aes(label = p_value_label, y = Mean + SEM + 0.1),
size = 3, color = "black", lineheight = 0.9)  # Add a bit more spacing above the error bars
View(accuracy_pvalue_results)
write.csv(final_performance_table, "final performance table.csv")
cran_packages
cran_packages <- c(cran_packages, "reshape2", "pheatmap", "RColorBrewer")
cran_packages
save(cran_packages, file = "data/cran_packages.rda")
devtools::document()
devtools::load_all()

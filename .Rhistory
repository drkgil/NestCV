# Generate the prediction object
log_roc_pred <- prediction(log_roc_prob_clean, Y_outer_test_num)
# Calculate performance metrics
log_roc_perf[[i]] <- performance(log_roc_pred, "tpr", "fpr")
log_auc <- performance(log_roc_pred, measure = "auc")
log_auc_value[[i]] <- log_auc@y.values[[1]]
log_aucpr <- performance(log_roc_pred, measure = "aucpr")
log_aucpr_value[[i]] <- log_aucpr@y.values[[1]]
log_pr_perf[[i]] <- performance(log_roc_pred, "prec", "rec")
# LASSO
lasso_roc$predictions[[i]] <- predict(lasso_outer_model, newx = X_outer_test, s = "lambda.min")
lasso_roc$labels[[i]] <- Y_outer_test
lasso_roc_predictions <- predict(lasso_outer_model, newx = X_outer_test, s = "lambda.min")
lasso_roc_pred <- prediction(lasso_roc_predictions, Y_outer_test)
lasso_roc_perf[[i]] <- performance(lasso_roc_pred, "tpr", "fpr")
lasso_auc <- performance(lasso_roc_pred, measure = "auc")
lasso_auc_value[[i]] <- lasso_auc@y.values[[1]]
lasso_aucpr <- performance(lasso_roc_pred, measure = "aucpr")
lasso_aucpr_value[[i]] <- lasso_aucpr@y.values[[1]]
lasso_pr_perf[[i]] <- performance(lasso_roc_pred, "prec", "rec")
# Elastic Net
en_roc$predictions[[i]] <- predict(en_outer_model, newx = X_outer_test, s = "lambda.min")
en_roc$labels[[i]] <- Y_outer_test
en_roc_predictions <- predict(en_outer_model, newx = X_outer_test, s = "lambda.min")
en_roc_pred <- prediction(en_roc_predictions, Y_outer_test)
en_roc_perf[[i]] <- performance(en_roc_pred, "tpr", "fpr")
en_auc <- performance(en_roc_pred, measure = "auc")
en_auc_value[[i]] <- en_auc@y.values[[1]]
en_aucpr <- performance(en_roc_pred, measure = "aucpr")
en_aucpr_value[[i]] <- en_aucpr@y.values[[1]]
en_pr_perf[[i]] <- performance(en_roc_pred, "prec", "rec")
# Random Forest
rf_roc_pred_ind <- predict(rf_outer_model, X_outer_test, type = "prob")
rf_roc$predictions[[i]] <- rf_roc_pred_ind[,2]
rf_roc$labels[[i]] <- Y_outer_test
rf_roc_predictions <- predict(rf_outer_model, X_outer_test, type = "prob")
rf_roc_pred <- prediction(rf_roc_predictions[,2], Y_outer_test)
rf_roc_perf[[i]] <- performance(rf_roc_pred, "tpr", "fpr")
rf_auc <- performance(rf_roc_pred, measure = "auc")
rf_auc_value[[i]] <- rf_auc@y.values[[1]]
rf_aucpr <- performance(rf_roc_pred, measure = "aucpr")
rf_aucpr_value[[i]] <- rf_aucpr@y.values[[1]]
rf_pr_perf[[i]] <- performance(rf_roc_pred, "prec", "rec")
# Classification Tree
ct_roc_pred_ind <- predict(ct_outer_model, data.frame(X_outer_test), type = "prob")
ct_roc$predictions[[i]] <- ct_roc_pred_ind[,2]
ct_roc$labels[[i]] <- Y_outer_test
ct_roc_predictions <- predict(ct_outer_model, data.frame(X_outer_test), type = "prob")
ct_roc_pred <- prediction(ct_roc_predictions[,2], Y_outer_test)
ct_roc_perf[[i]] <- performance(ct_roc_pred, "tpr", "fpr")
ct_auc <- performance(ct_roc_pred, measure = "auc")
ct_auc_value[[i]] <- ct_auc@y.values[[1]]
ct_aucpr <- performance(ct_roc_pred, measure = "aucpr")
ct_aucpr_value[[i]] <- ct_aucpr@y.values[[1]]
ct_pr_perf[[i]] <- performance(ct_roc_pred, "prec", "rec")
# adaBoost
ada_roc$predictions[[i]] <- predict(ada_outer_model, X_outer_test, type = "prob")
ada_roc$labels[[i]] <- Y_outer_test
ada_roc_predictions <- predict(ada_outer_model, X_outer_test, type = "prob")
ada_roc_pred <- prediction(ada_roc_predictions, Y_outer_test)
ada_roc_perf[[i]] <- performance(ada_roc_pred, "tpr", "fpr")
ada_auc <- performance(ada_roc_pred, measure = "auc")
ada_auc_value[[i]] <- ada_auc@y.values[[1]]
ada_aucpr <- performance(ada_roc_pred, measure = "aucpr")
ada_aucpr_value[[i]] <- ada_aucpr@y.values[[1]]
ada_pr_perf[[i]] <- performance(ada_roc_pred, "prec", "rec")
# XGBoost
xgb_roc$predictions[[i]] <- predict(xgb_outer_model, dtest, type = "prob")
xgb_roc$labels[[i]] <- Y_outer_test
xgb_roc_predictions <- predict(xgb_outer_model, dtest, type = "prob")
xgb_roc_pred <- prediction(xgb_roc_predictions, Y_outer_test)
xgb_roc_perf[[i]] <- performance(xgb_roc_pred, "tpr", "fpr")
xgb_auc <- performance(xgb_roc_pred, measure = "auc")
xgb_auc_value[[i]] <- xgb_auc@y.values[[1]]
xgb_aucpr <- performance(xgb_roc_pred, measure = "aucpr")
xgb_aucpr_value[[i]] <- xgb_aucpr@y.values[[1]]
xgb_pr_perf[[i]] <- performance(xgb_roc_pred, "prec", "rec")
# pROC can do statistics comparing 2 auc!!
test_nb <- pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(nb_roc_pred@predictions)))
test_log <- pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(log_roc_pred@predictions)))
test_lasso <- pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(lasso_roc_pred@predictions)))
test_en <- pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(en_roc_pred@predictions)))
test_rf <- pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(rf_roc_pred@predictions)))
test_ct <- pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(ct_roc_pred@predictions)))
test_ada <- pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(ada_roc_pred@predictions)))
test_xgb <- pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(xgb_roc_pred@predictions)))
# Your 8 AUC objects
auc_models <- list(
nb = test_nb,
log = test_log,
lasso = test_lasso,
en = test_en,
rf = test_rf,
ct = test_ct,
ada = test_ada,
xgb = test_xgb
)
# Create a matrix to store p-values
auc_model_names <- names(auc_models)
p_values <- matrix(NA, nrow = length(auc_models), ncol = length(auc_models), dimnames = list(auc_model_names, auc_model_names))
# Pairwise ROC tests with additional checks
for (m in seq_along(auc_models)) {
for (n in seq_along(auc_models)) {
if (m < n) {  # Only test pairs once
# Check for NaN or Inf in AUCs
if (is.nan(auc_models[[m]]) || is.nan(auc_models[[n]]) ||
is.infinite(auc_models[[m]]) || is.infinite(auc_models[[n]])) {
message("Skipping due to NaN/Inf for models: ", auc_model_names[m], " vs ", auc_model_names[n])
next
}
# Skip if both AUCs are 1
if (auc_models[[m]] == 1 && auc_models[[n]] == 1) {
message("Skipping due to both AUCs = 1 for models: ", auc_model_names[m], " vs ", auc_model_names[n])
next
}
# Run roc.test with error handling
test_result <- tryCatch({
pROC::roc.test(
as.numeric(Y_outer_test),
as.numeric(unlist(get(paste0(auc_model_names[m], "_roc_pred"))@predictions)),
as.numeric(unlist(get(paste0(auc_model_names[n], "_roc_pred"))@predictions))
)
}, error = function(e) {
message("Comparison failed for models: ", auc_model_names[m], " vs ", auc_model_names[n])
message("Error: ", e$message)
return(NULL)
})
# Save p-value
if (!is.null(test_result)) {
p_values[m, n] <- test_result$p.value
print(paste("Storing p-value for models:", auc_model_names[m], "vs", auc_model_names[n], "p-value:", test_result$p.value))
} else {
p_values[m, n] <- NA
}
}
}
}
# Convert to a tidy data frame
p_values_df <- as.data.frame(as.table(p_values))
p_values_df <- p_values_df[!is.na(p_values_df$Freq), ]
colnames(p_values_df) <- c("Model1", "Model2", "p.value")
# Convert p-value matrix to a data frame for plotting
p_values[lower.tri(p_values, diag = TRUE)] <- NA # Keep only upper triangle
p_values_melted <- melt(p_values, na.rm = TRUE)
# Add significance stars based on p-value
p_values_melted$significance <- with(p_values_melted, ifelse(
value < 0.001, "***",
ifelse(value < 0.01, "**",
ifelse(value < 0.05, "*", ""))
))
# Display p-values or asterisks
p_values_melted$label <- ifelse(
p_values_melted$significance != "",
p_values_melted$significance,  # show stars if significant
sprintf("%.3f", p_values_melted$value) # show p-value otherwise
)
# Save each outer fold iteration of p_values_melted:
auc_p_values_melted[[i]] <- p_values_melted
performance_table <- data.frame(
Metric = c("Accuracy", "Sensitivity", "Specificity", "Precision", "F1", "MCC", "AUC", "AUCPR", "RMSE", "TP", "FP", "FN", "TN"),
`Naive Bayes` = c(
con_NB$overall["Accuracy"],
con_NB$byClass["Sensitivity"],
con_NB$byClass["Specificity"],
con_NB$byClass["Precision"],
con_NB$byClass["F1"],
MCC_NB,
pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(nb_roc_pred@predictions))),
unlist((performance(nb_roc_pred, measure = "aucpr")@y.values)),
unlist((performance(nb_roc_pred, measure = "rmse")@y.values)),
con_NB$table[[4]],
con_NB$table[[2]],
con_NB$table[[3]],
con_NB$table[[1]]
),
`Logistic` = c(
con_Logistic$overall["Accuracy"],
con_Logistic$byClass["Sensitivity"],
con_Logistic$byClass["Specificity"],
con_Logistic$byClass["Precision"],
con_Logistic$byClass["F1"],
MCC_Logistic,
pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(log_roc_pred@predictions))),
unlist((performance(log_roc_pred, measure = "aucpr")@y.values)),
unlist((performance(log_roc_pred, measure = "rmse")@y.values)),
con_Logistic$table[[4]],
con_Logistic$table[[2]],
con_Logistic$table[[3]],
con_Logistic$table[[1]]
),
`LASSO` = c(
con_lasso$overall["Accuracy"],
con_lasso$byClass["Sensitivity"],
con_lasso$byClass["Specificity"],
con_lasso$byClass["Precision"],
con_lasso$byClass["F1"],
MCC_Lasso,
pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(lasso_roc_pred@predictions))),
unlist((performance(lasso_roc_pred, measure = "aucpr")@y.values)),
unlist((performance(lasso_roc_pred, measure = "rmse")@y.values)),
con_lasso$table[[4]],
con_lasso$table[[2]],
con_lasso$table[[3]],
con_lasso$table[[1]]
),
`EN` = c(
con_en$overall["Accuracy"],
con_en$byClass["Sensitivity"],
con_en$byClass["Specificity"],
con_en$byClass["Precision"],
con_en$byClass["F1"],
MCC_En,
pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(en_roc_pred@predictions))),
unlist((performance(en_roc_pred, measure = "aucpr")@y.values)),
unlist((performance(en_roc_pred, measure = "rmse")@y.values)),
con_en$table[[4]],
con_en$table[[2]],
con_en$table[[3]],
con_en$table[[1]]
),
`RF` = c(
con_RF$overall["Accuracy"],
con_RF$byClass["Sensitivity"],
con_RF$byClass["Specificity"],
con_RF$byClass["Precision"],
con_RF$byClass["F1"],
MCC_RF,
pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(rf_roc_pred@predictions))),
unlist((performance(rf_roc_pred, measure = "aucpr")@y.values)),
unlist((performance(rf_roc_pred, measure = "rmse")@y.values)),
con_RF$table[[4]],
con_RF$table[[2]],
con_RF$table[[3]],
con_RF$table[[1]]
),
`Class Tree` = c(
con_classTree$overall["Accuracy"],
con_classTree$byClass["Sensitivity"],
con_classTree$byClass["Specificity"],
con_classTree$byClass["Precision"],
con_classTree$byClass["F1"],
MCC_classTree,
pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(ct_roc_pred@predictions))),
unlist((performance(ct_roc_pred, measure = "aucpr")@y.values)),
unlist((performance(ct_roc_pred, measure = "rmse")@y.values)),
con_classTree$table[[4]],
con_classTree$table[[2]],
con_classTree$table[[3]],
con_classTree$table[[1]]
),
`adaBoost` = c(
con_adaBoost$overall["Accuracy"],
con_adaBoost$byClass["Sensitivity"],
con_adaBoost$byClass["Specificity"],
con_adaBoost$byClass["Precision"],
con_adaBoost$byClass["F1"],
MCC_adaBoost,
pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(ada_roc_pred@predictions))),
unlist((performance(ada_roc_pred, measure = "aucpr")@y.values)),
unlist((performance(ada_roc_pred, measure = "rmse")@y.values)),
con_adaBoost$table[[4]],
con_adaBoost$table[[2]],
con_adaBoost$table[[3]],
con_adaBoost$table[[1]]
),
`XGBoost` = c(
con_XGB$overall["Accuracy"],
con_XGB$byClass["Sensitivity"],
con_XGB$byClass["Specificity"],
con_XGB$byClass["Precision"],
con_XGB$byClass["F1"],
MCC_XGB,
pROC::auc(as.numeric(Y_outer_test), as.numeric(unlist(xgb_roc_pred@predictions))),
unlist((performance(xgb_roc_pred, measure = "aucpr")@y.values)),
unlist((performance(xgb_roc_pred, measure = "rmse")@y.values)),
con_XGB$table[[4]],
con_XGB$table[[2]],
con_XGB$table[[3]],
con_XGB$table[[1]]
)
)
# Store the result for this iteration
all_metrics[[i]] <- performance_table
# Random Forest confusion arrays
rf_confusion_array[[i]] <- rf_outer_model$confusion
# Store outer results
outer_results[[i]] <- list(
actual = Y_outer_test,
nb_predictions = nb_outer_predictions,
log_predictions = log_outer_predictions,
lasso_predictions = lasso_outer_predictions,
en_predictiosn = en_outer_predictions,
rf_predictions = rf_outer_predictions,
ct_predictions = ct_outer_predictions,
ada_predictions = ada_outer_predictions,
xgb_predictions = xgb_outer_predictions
)
# Feature Importance
# Naive Bayes
importance_scores <- sapply(nb_outer_model$tables, function(tbl) {
if (is.matrix(tbl)) {  # Only process numeric features
means <- tbl[1, ]  # Mean for each class
sds <- tbl[2, ]    # SD for each class
# Avoid division by zero
sds[sds == 0] <- 1e-6
# Compute log-likelihood components (ignoring actual x values)
log_likelihoods <- - (means^2) / (2 * sds^2) - log(sds)
return(sd(log_likelihoods, na.rm = TRUE))  # SD across classes
} else {
return(NA)  # Skip categorical variables
}
})
nb_imp_df <- as.data.frame(importance_scores)
nb_imp_df$Feature <- rownames(nb_imp_df)
# Reorder the dataframe by importance_scores
nb_imp_df <- nb_imp_df[order(nb_imp_df$importance_scores, decreasing = TRUE), ]
nb_imp_df_reordered[[i]] <- nb_imp_df
# Extract the reordered feature names from the row names
nb_imp_features <- nb_imp_df$Feature
# Logistic Regression
# Coefficient Plot: Effect of Predictors
# Coefficients for the logistic regression model
coef_df <- data.frame(
Variable = names(coef(log_outer_model)),
Coefficient = coef(log_outer_model)
)
#Removing Intercept as it overshadows the entire plot otherwise
coef_df <- data.frame(coef_df[-1,])
# Sorting the coefficients by their absolute value in descending order
coef_df <- coef_df[order(abs(coef_df$Coefficient), decreasing = TRUE), ]
log_coef_df[[i]] <- coef_df
# Extract the reordered variable names
log_imp_features <- coef_df$Variable
# LASSO
# Feature Importance by the lambda min coefficient
# note: lasso_outer_model is the same as cvfit in the rmarkdown file
# get the value of lambda.min and the model coefficients at that value of λ
coef_min <- as.matrix(coef(lasso_outer_model, s = "lambda.min"))
# Convert the coefficients into a data frame
coef_min_df <- as.data.frame(as.matrix(coef_min))
selected_coefs <- as.matrix(coef_min[2:nrow(coef_min),]) # get everything but the intercept
sorted_coefs <- selected_coefs[order(abs(selected_coefs), decreasing = TRUE),]			# alternatively use this for saving & graphing
sorted_coefs_names <- data.frame(Feature = names(sorted_coefs), Importance = sorted_coefs)
# Filter rows where Importance is non-zero
non_zero_coefs <- sorted_coefs_names[sorted_coefs_names$Importance != 0, ]
lasso_non_zero_coefs[[i]] <- non_zero_coefs
# Extract Feature names
lasso_imp_features <- non_zero_coefs$Feature
# Elastic Net
# Feature Importance by the lambda min coefficient
# get the value of lambda.min and the model coefficients at that value of λ
en_coef_min <- as.matrix(coef(en_outer_model, s = "lambda.min"))
# Convert the coefficients into a data frame
en_coef_min_df <- as.data.frame(as.matrix(en_coef_min))
en_selected_coefs <- as.matrix(en_coef_min[2:nrow(en_coef_min),]) # get everything but the intercept
en_sorted_coefs <- en_selected_coefs[order(abs(en_selected_coefs), decreasing = TRUE),]
en_sorted_coefs_names <- data.frame(Feature = names(en_sorted_coefs), Importance = en_sorted_coefs)
# Filter rows where Importance is non-zero
en_non_zero_coefs <- en_sorted_coefs_names[en_sorted_coefs_names$Importance != 0, ]
en_non_zero_coefs_all[[i]] <- en_non_zero_coefs
# Extract Feature names
en_imp_features <- en_non_zero_coefs$Feature
# Random Forest
# Extract feature importance
rf_importance_df <- data.frame(Feature = rownames(rf_outer_model$importance),
Importance = rf_outer_model$importance[, "MeanDecreaseAccuracy"])
rf_importance_df_reordered <- rf_importance_df[order(rf_importance_df$Importance, decreasing = TRUE), ]
# Filter rows where Importance is not-negative
rf_non_zero <- rf_importance_df_reordered[rf_importance_df_reordered$Importance > 0, ]
rf_nonzero_importance[[i]] <- rf_non_zero
rf_imp_features <- rf_non_zero$Feature
# Classification Tree
ct_imp <- ct_outer_model[["variable.importance"]]
ct_imp <- data.frame(Feature = names(ct_imp))
ct_imp_df[[i]] <- ct_imp
ct_imp_features <-  ct_imp$Feature
# adaBoost
frame <- ada_outer_model[["trees"]][[1]][["frame"]]
split_nodes <- frame[frame$var != "<leaf>", ]
# Check if there are any split nodes before aggregating
if (nrow(split_nodes) > 0) {
# Summarize total impurity reduction per feature
ada_feature_importance <- aggregate(dev ~ var, data = split_nodes, sum)
# Sort by importance
ada_feature_importance <- ada_feature_importance[order(-ada_feature_importance$dev), ]
ada_feature_importance_sorted[[i]] <- ada_feature_importance
ada_imp_features <- ada_feature_importance$var
} else {
# No splits; assign NA
ada_feature_importance_sorted[[i]] <- NA
ada_imp_features <- NA
}
# XGBoost
xgb_sorted_imp <- xgb.importance(model = xgb_outer_model) # if this model in a round doesn't work, this will be an empty data table
xgb_imp[[i]] <- xgb_sorted_imp
xgb_imp_features <- xgb_sorted_imp$Feature
# Feature Rankings
# The following is a heatmap indicating the number of times each feature ranked 1st through `r length(genes)` number of features.
# Helper to get safe length even if object is NA
safe_length <- function(x) {
if (is.null(x) || all(is.na(x))) {
return(0)
} else {
return(length(x))
}
}
# Compute the max length safely
max_features <- max(
safe_length(nb_imp_features),
safe_length(log_imp_features),
safe_length(lasso_imp_features),
safe_length(en_imp_features),
safe_length(rf_imp_features),
safe_length(ct_imp_features),
safe_length(ada_imp_features),
safe_length(xgb_imp_features)
)
# Updated padding function to handle NA
pad_features <- function(features, max_length) {
if (is.null(features) || all(is.na(features))) {
return(rep(NA, max_length))
} else {
length(features) <- max_length
return(features)
}
}
# Apply the padding function to all feature lists
nb_imp_features <- pad_features(nb_imp_features, max_features)
log_imp_features <- pad_features(log_imp_features, max_features)
lasso_imp_features <- pad_features(lasso_imp_features, max_features)
en_imp_features <- pad_features(en_imp_features, max_features)
rf_imp_features <- pad_features(rf_imp_features, max_features)
ct_imp_features <- pad_features(ct_imp_features, max_features)
ada_imp_features <- pad_features(ada_imp_features, max_features)
xgb_imp_features <- pad_features(xgb_imp_features, max_features)
# Combine into data frame
combined_features <- data.frame(
NB = nb_imp_features,
LR = log_imp_features,
LASSO = lasso_imp_features,
EN = en_imp_features,
RF = rf_imp_features,
CT = ct_imp_features,
ADA = ada_imp_features,
XGB = xgb_imp_features,
stringsAsFactors = FALSE
)
combined_features_all[[i]] <- combined_features
# Get the ranks for each feature
ranked_combined_features <- apply(combined_features, 2, function(x) rank(x, ties.method = "first"))
# Extract all feature names from any column and sort them alphabetically
sorted_feature_names <- sort(combined_features[, 1])
# report the number of times a character in sorted_feature_names appears in a row in combined_features
# Create a new matrix to store the counts
count_matrix <- matrix(0, nrow = nrow(combined_features), ncol = length(sorted_feature_names))
# Set row names to match the rows of combined_features
rownames(count_matrix) <- rownames(combined_features)
# Set column names to be the features in sorted_feature_names
colnames(count_matrix) <- sorted_feature_names
# Loop through each feature and count occurrences in each row, skipping NAs
for (k in seq_along(sorted_feature_names)) {
feature <- sorted_feature_names[k]
count_matrix[, k] <- apply(combined_features, 1, function(row) {
# Count occurrences, ignoring NA
sum(!is.na(row) & row == feature)
})
}
# Convert the count_matrix to a dataframe for easier visualization
imp_feature_count_df[[i]] <- as.data.frame(count_matrix)
setTxtProgressBar(pbo,i)
} # End of Outer Loop
devtools::document()
devtools::load_all()
nested_cv()
cran_packages <- c(cran_packages, "reshape2")
devtools::document()
devtools::load_all()
load_packages()
nested_cv()
library(available)
install.packages("available")
library(available)
available::suggest("nested cross validation of subset features")
available::suggest("nested cross validation of subset features")
available::suggest("nested cross validation")
available(validationR)
available("overviewR")
available::available("validationR", browse = FALSE)
suggest("nested cv")
available("nestedR")
available("NestCV", browse = F)
View(combined_features_all)
View(combined_features_all)
View(ct_aucpr_value)
View(ct_pr_perf)
View(nested_cv)
source(system.file("scripts/nested_cv_setup.R", package = "yourpackagename"))
source(system.file("scripts/run_nested_cv.R", package = "MyToolbox"))
source(system.file("scripts/run_nested_cv.R", package = "MyToolbox"), local = .GlobalEnv)
devtools::document()
devtools::load_all()
run_nested_cv()
devtools::document()
devtools::load_all()
run_nested_cv()
View(ada_auc)
git remote add origin https://github.com/drkgil/NestCV.git
git remote add origin https://github.com/drkgil/NestCV.git
Sys.which("git")
Sys.which("git")
Sys.which("git")
Sys.which("git")

try(plot(perf_log, avg = "vertical", add = TRUE, lwd = 2, col = "black"), silent = TRUE)
}
if (!is.null(perf_log_pooled) &&
length(na.omit(perf_log_pooled@x.values[[1]])) >= 2 &&
length(na.omit(perf_log_pooled@y.values[[1]])) >= 2) {
try(plot(perf_log_pooled, add = TRUE, col = "blue", lwd = 3), silent = TRUE)
}
legend("bottomright",
legend = c("Individual Folds",
paste("Average ROC (AUC =", round(ave_auc_log, 2), "±", round(sd_auc_log, 2), ")"),
paste("Pooled ROC (AUC =", round(test_log, 2), ")")),
col = c("gray", "black", "blue"),
lwd = c(1, 2, 3))
title(main = "Logistic Regression ROC Curves: Individual, Averaged, and Pooled")
} else {
warning("Not enough data to plot Logistic Regression ROC curves")
}
plot(perf_lasso, col = "gray")  # Individual ROC curves
plot(perf_lasso, avg = "vertical", add = TRUE, lwd = 2, col = "black")  # Averaged ROC (fold-wise)
plot(perf_lasso_pooled, add = TRUE, col = "blue", lwd = 3)  # Pooled ROC
# Add a legend
legend("bottomright",
legend = c("Individual Folds",
paste("Average ROC (AUC =", round(ave_auc_lasso, 2), "±", round(sd_auc_lasso, 2), ")"),
paste("Pooled ROC (AUC =", round(test_lasso, 2), ")")),
col = c("gray", "black", "blue"),
lwd = c(1, 2, 3))
# Add plot title and labels (optional)
title(main = "LASSO ROC Curves: Individual, Averaged, and Pooled")
plot(perf_en, col = "gray")  # Individual ROC curves
plot(perf_en, avg = "vertical", add = TRUE, lwd = 2, col = "black")  # Averaged ROC (fold-wise)
plot(perf_en_pooled, add = TRUE, col = "blue", lwd = 3)  # Pooled ROC
# Add a legend
legend("bottomright",
legend = c("Individual Folds",
paste("Average ROC (AUC =", round(ave_auc_en, 2), "±", round(sd_auc_en, 2), ")"),
paste("Pooled ROC (AUC =", round(test_en, 2), ")")),
col = c("gray", "black", "blue"),
lwd = c(1, 2, 3))
# Add plot title and labels (optional)
title(main = "Elastic Net ROC Curves: Individual, Averaged, and Pooled")
plot(perf_rf, col = "gray")  # Individual ROC curves
plot(perf_rf, avg = "vertical", add = TRUE, lwd = 2, col = "black")  # Averaged ROC (fold-wise)
plot(perf_rf_pooled, add = TRUE, col = "blue", lwd = 3)  # Pooled ROC
# Add a legend
legend("bottomright",
legend = c("Individual Folds",
paste("Average ROC (AUC =", round(ave_auc_rf, 2), "±", round(sd_auc_rf, 2), ")"),
paste("Pooled ROC (AUC =", round(test_rf, 2), ")")),
col = c("gray", "black", "blue"),
lwd = c(1, 2, 3))
# Add plot title and labels (optional)
title(main = "Random Forest ROC Curves: Individual, Averaged, and Pooled")
# Classification Trees ROC curves
if (!is.null(perf_ct) &&
length(na.omit(perf_ct@x.values[[1]])) >= 2 &&
length(na.omit(perf_ct@y.values[[1]])) >= 2) {
plot(perf_ct, col = "gray")  # Individual ROC curves
if (!is.null(perf_ct) &&
length(na.omit(perf_ct@x.values[[1]])) >= 2 &&
length(na.omit(perf_ct@y.values[[1]])) >= 2) {
try(plot(perf_ct, avg = "vertical", add = TRUE, lwd = 2, col = "black"), silent = TRUE)
}
if (!is.null(perf_ct_pooled) &&
length(na.omit(perf_ct_pooled@x.values[[1]])) >= 2 &&
length(na.omit(perf_ct_pooled@y.values[[1]])) >= 2) {
try(plot(perf_ct_pooled, add = TRUE, col = "blue", lwd = 3), silent = TRUE)
}
legend("bottomright",
legend = c("Individual Folds",
paste("Average ROC (AUC =", round(ave_auc_ct, 2), "±", round(sd_auc_ct, 2), ")"),
paste("Pooled ROC (AUC =", round(test_ct, 2), ")")),
col = c("gray", "black", "blue"),
lwd = c(1, 2, 3))
title(main = "Classification Trees ROC Curves: Individual, Averaged, and Pooled")
} else {
warning("Not enough data to plot Classification Tree ROC curves")
}
plot(perf_ada, col = "gray")  # Individual ROC curves
plot(perf_ada, avg = "vertical", add = TRUE, lwd = 2, col = "black")  # Averaged ROC (fold-wise)
plot(perf_ada_pooled, add = TRUE, col = "blue", lwd = 3)  # Pooled ROC
# Add a legend
legend("bottomright",
legend = c("Individual Folds",
paste("Average ROC (AUC =", round(ave_auc_ada, 2), "±", round(sd_auc_ada, 2), ")"),
paste("Pooled ROC (AUC =", round(test_ada, 2), ")")),
col = c("gray", "black", "blue"),
lwd = c(1, 2, 3))
# Add plot title and labels (optional)
title(main = "Adaptive Boost ROC Curves: Individual, Averaged, and Pooled")
plot(perf_xgb, col = "gray")  # Individual ROC curves
plot(perf_xgb, avg = "vertical", add = TRUE, lwd = 2, col = "black")  # Averaged ROC (fold-wise)
plot(perf_xgb_pooled, add = TRUE, col = "blue", lwd = 3)  # Pooled ROC
# Add a legend
legend("bottomright",
legend = c("Individual Folds",
paste("Average ROC (AUC =", round(ave_auc_xgb, 2), "±", round(sd_auc_xgb, 2), ")"),
paste("Pooled ROC (AUC =", round(test_xgb, 2), ")")),
col = c("gray", "black", "blue"),
lwd = c(1, 2, 3))
# Add plot title and labels (optional)
title(main = "Extreme Gradient Boosting ROC Curves: Individual, Averaged, and Pooled")
# Add the top ROC curves of your choosing together:
# Define colors from the Blues palette for the 8 curves
colors <- brewer.pal(8, "Paired")
# Plot the first model's ROC curve
plot(perf_nb_pooled, col = colors[1], lwd = 2, main = "Model Comparison: ROC Curves",
xlim = c(0, 1), ylim = c(0, 1),
xlab = "False Positive Rate", ylab = "True Positive Rate")
# Add other models' ROC curves
plot(perf_log_pooled, col = colors[2], lwd = 2, add = TRUE)
plot(perf_lasso_pooled, col = colors[3], lwd = 2, add = TRUE)
plot(perf_en_pooled, col = colors[4], lwd = 2, add = TRUE)
plot(perf_rf_pooled, col = colors[5], lwd = 2, add = TRUE)
plot(perf_ct_pooled, col = colors[6], lwd = 2, add = TRUE)
plot(perf_ada_pooled, col = colors[7], lwd = 2, add = TRUE)
plot(perf_xgb_pooled, col = colors[8], lwd = 2, add = TRUE)
lines(x=c(0, 1), y=c(0, 1), col="grey", lty=2) # add diagonal gray line
# Add the legend
legend("bottomright",
legend = c(paste("Naive Bayes ROC (AUC =", round(test_nb, 3), ")"),
paste("Logistic ROC (AUC =", round(test_log, 3), ")"),
paste("Lasso ROC (AUC =", round(test_lasso, 3), ")"),
paste("ElasticNet ROC (AUC =", round(test_en, 3), ")"),
paste("Random Forest ROC (AUC =", round(test_rf, 3), ")"),
paste("Class Tree ROC (AUC =", round(test_ct, 3), ")"),
paste("AdaBoost ROC (AUC =", round(test_ada, 3), ")"),
paste("XGBoost ROC (AUC =", round(test_xgb, 3), ")")
),
col = colors,
lwd = 2)
# Statistically test the pooled AUCs against each other:
# Your 8 AUC objects
auc_models <- list(
nb = test_nb,
log = test_log,
lasso = test_lasso,
en = test_en,
rf = test_rf,
ct = test_ct,
ada = test_ada,
xgb = test_xgb
)
# Create a matrix to store p-values
auc_model_names <- names(auc_models)
p_values <- matrix(NA, nrow = length(auc_models), ncol = length(auc_models), dimnames = list(auc_model_names, auc_model_names))
# Pairwise ROC tests with additional checks
for (m in seq_along(auc_models)) {
for (n in seq_along(auc_models)) {
if (m < n) { # Only test pairs once
# Check for NaN or infinite values in predictions
if (any(is.nan(unlist(auc_models[[m]]))) ||
any(is.nan(unlist(auc_models[[n]]))) ||
any(is.infinite(unlist(auc_models[[m]]))) ||
any(is.infinite(unlist(auc_models[[n]])))) {
print(paste("Skipping due to NaN/Inf for models:", auc_model_names[m], "vs", auc_model_names[n]))
next  # Skip this iteration if NaN or Inf detected
}
# Get AUC values
auc_m <- auc_models[[m]]
auc_n <- auc_models[[n]]
# Skip if both AUCs are 1 (causes roc.test error)
if (auc_m == 1 && auc_n == 1) {
print(paste("Skipping due to both AUCs = 1 for models:", auc_model_names[m], "vs", auc_model_names[n]))
next
}
# Perform the test if valid
test_result <- pROC::roc.test(auc_models[[m]], auc_models[[n]])
print(test_result) # important to get the stats!
print(paste("Storing p-value for models:", auc_model_names[m], "vs", auc_model_names[n], "p-value:", test_result$p.value))
p_values[m, n] <- test_result$p.value
}
}
}
# Convert to a tidy data frame
p_values_df <- as.data.frame(as.table(p_values))
p_values_df <- p_values_df[!is.na(p_values_df$Freq), ]
colnames(p_values_df) <- c("Model1", "Model2", "p.value")
# Convert p-value matrix to a data frame for plotting
p_values[lower.tri(p_values, diag = TRUE)] <- NA # Keep only upper triangle
p_values_melted <- melt(p_values, na.rm = TRUE)
# Add significance stars based on p-value
p_values_melted$significance <- with(p_values_melted, ifelse(
value < 0.001, "***",
ifelse(value < 0.01, "**",
ifelse(value < 0.05, "*", ""))
))
# Display p-values or asterisks
p_values_melted$label <- ifelse(
p_values_melted$significance != "",
p_values_melted$significance,  # show stars if significant
sprintf("%.3f", p_values_melted$value) # show p-value otherwise
)
# Plot DeLong's Test comparing AUCs of ML Models
ggplot(p_values_melted, aes(Var1, Var2, fill = value)) +
geom_tile(color = "white") +
geom_text(aes(label = label), color = "black", size = 4) +
scale_fill_gradient(low = "lightblue", high = "red", na.value = "white", name = "p-value") +
theme_minimal() +
labs(x = "", y = "", title = "DeLong's Test Comparing AUCs of ML Models") +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
panel.grid = element_blank()
)
# Find the best cutoff value for a model using the pooled performance
# Based on Youden's J: Jouden index is a single statistic that captures the performance of a dichotomous diagnostic test
nb_sands <- performance(pred_nb_pooled, "sens", "spec")  # you save the sensitivity and specificity from this object
nb_sens <- unlist(nb_sands@y.values)
nb_spec <- unlist(nb_sands@x.values)
nb_cutoffs <- unlist(nb_sands@alpha.values)
nb_J_scores <- nb_sens + nb_spec - 1
nb_best_index <- which.max(nb_J_scores)
nb_best_cutoff <- nb_cutoffs[nb_best_index] 	# use for the sensitivity specificity plot and accuracy vs cutoff plot
print(paste("Best cutoff based on Youden's J: ", nb_best_cutoff))
# You can plot the accuracy vs. cutoff to make sure the point where accuracy is highest matches your best cutoff from Youden's J.
# Plot accuracy over cutoffs
# If the red vertical line (best cutoff) aligns with the peak of your accuracy curve,
# you can be even more confident that the best_cutoff is the optimal one!
accuracy <- (nb_sens + nb_spec) / 2  # Average of sensitivity and specificity
plot(nb_cutoffs, accuracy, type = "l", xlab = "Cutoff", ylab = "Accuracy", main = "Naive Bayes Accuracy vs. Cutoff")
abline(v = nb_best_cutoff, col = "red", lty = 2)  # Highlight the best cutoff
# Obtain sensitivity and specificity coordinates from the pooled ROC object
nb_best_sens <- (unlist(perf_nb_pooled@y.values))[nb_best_index]
nb_best_spec <- (unlist(perf_nb_pooled@x.values))[nb_best_index]
# Plot the Pooled ROC curve and add a point where the best cutoff is, i.e., Youden's Index
plot(perf_nb_pooled, colorize = TRUE)
points(nb_best_spec, nb_best_sens, col="black", pch=16, cex=1.5)
text(nb_best_spec, nb_best_sens, labels="Youden's Index", pos=4, col="black")
title(main = "Naive Bayes Pooled ROC")
# Repeat the same for other models:
log_sands <- performance(pred_log_pooled, "sens", "spec")  # you save the sensitivity and specificity from this object
log_sens <- unlist(log_sands@y.values)
log_spec <- unlist(log_sands@x.values)
log_cutoffs <- unlist(log_sands@alpha.values)
log_J_scores <- log_sens + log_spec - 1
log_best_index <- which.max(log_J_scores)
log_best_cutoff <- log_cutoffs[log_best_index] 	# use for the sensitivity specificity plot and accuracy vs cutoff plot
print(paste("Best cutoff based on Youden's J: ", log_best_cutoff))
accuracy <- (log_sens + log_spec) / 2  # Average of sensitivity and specificity
plot(log_cutoffs, accuracy, type = "l", xlab = "Cutoff", ylab = "Accuracy", main = "Logistic Regression Accuracy vs. Cutoff")
abline(v = log_best_cutoff, col = "red", lty = 2)  # Highlight the best cutoff
# Obtain sensitivity and specificity coordinates from the pooled ROC object
log_best_sens <- (unlist(perf_log_pooled@y.values))[log_best_index]
log_best_spec <- (unlist(perf_log_pooled@x.values))[log_best_index]
# Plot the Pooled ROC curve and add a point where the best cutoff is, i.e., Youden's Index
plot(perf_log_pooled, colorize = TRUE)
points(log_best_spec, log_best_sens, col="black", pch=16, cex=1.5)
text(log_best_spec, log_best_sens, labels="Youden's Index", pos=4, col="black")
title(main = "Logistic Regression Pooled ROC")
lasso_sands <- performance(pred_lasso_pooled, "sens", "spec")
lasso_sens <- unlist(lasso_sands@y.values)
lasso_spec <- unlist(lasso_sands@x.values)
lasso_cutoffs <- unlist(lasso_sands@alpha.values)
lasso_J_scores <- lasso_sens + lasso_spec - 1
lasso_best_index <- which.max(lasso_J_scores)
lasso_best_cutoff <- lasso_cutoffs[lasso_best_index]
print(paste("Best cutoff based on Youden's J: ", lasso_best_cutoff))
accuracy <- (lasso_sens + lasso_spec) / 2
plot(lasso_cutoffs, accuracy, type = "l", xlab = "Cutoff", ylab = "Accuracy", main = "LASSO Accuracy vs. Cutoff")
abline(v = lasso_best_cutoff, col = "red", lty = 2)
lasso_best_sens <- (unlist(perf_lasso_pooled@y.values))[lasso_best_index]
lasso_best_spec <- (unlist(perf_lasso_pooled@x.values))[lasso_best_index]
plot(perf_lasso_pooled, colorize = TRUE)
points(lasso_best_spec, lasso_best_sens, col="black", pch=16, cex=1.5)
text(lasso_best_spec, lasso_best_sens, labels="Youden's Index", pos=4, col="black")
title(main = "LASSO Pooled ROC")
en_sands <- performance(pred_en_pooled, "sens", "spec")
en_sens <- unlist(en_sands@y.values)
en_spec <- unlist(en_sands@x.values)
en_cutoffs <- unlist(en_sands@alpha.values)
en_J_scores <- en_sens + en_spec - 1
en_best_index <- which.max(en_J_scores)
en_best_cutoff <- en_cutoffs[en_best_index]
print(paste("Best cutoff based on Youden's J: ", en_best_cutoff))
accuracy <- (en_sens + en_spec) / 2
plot(en_cutoffs, accuracy, type = "l", xlab = "Cutoff", ylab = "Accuracy", main = "Elastic Net Accuracy vs. Cutoff")
abline(v = en_best_cutoff, col = "red", lty = 2)
en_best_sens <- (unlist(perf_en_pooled@y.values))[en_best_index]
en_best_spec <- (unlist(perf_en_pooled@x.values))[en_best_index]
plot(perf_en_pooled, colorize = TRUE)
points(en_best_spec, en_best_sens, col="black", pch=16, cex=1.5)
text(en_best_spec, en_best_sens, labels="Youden's Index", pos=4, col="black")
title(main = "Elastic Net Pooled ROC")
rf_sands <- performance(pred_rf_pooled, "sens", "spec")
rf_sens <- unlist(rf_sands@y.values)
rf_spec <- unlist(rf_sands@x.values)
rf_cutoffs <- unlist(rf_sands@alpha.values)
rf_J_scores <- rf_sens + rf_spec - 1
rf_best_index <- which.max(rf_J_scores)
rf_best_cutoff <- rf_cutoffs[rf_best_index]
print(paste("Best cutoff based on Youden's J: ", rf_best_cutoff))
accuracy <- (rf_sens + rf_spec) / 2
plot(rf_cutoffs, accuracy, type = "l", xlab = "Cutoff", ylab = "Accuracy", main = "Random Forest Accuracy vs. Cutoff")
abline(v = rf_best_cutoff, col = "red", lty = 2)
rf_best_sens <- (unlist(perf_rf_pooled@y.values))[rf_best_index]
rf_best_spec <- (unlist(perf_rf_pooled@x.values))[rf_best_index]
plot(perf_rf_pooled, colorize = TRUE)
points(rf_best_spec, rf_best_sens, col="black", pch=16, cex=1.5)
text(rf_best_spec, rf_best_sens, labels="Youden's Index", pos=4, col="black")
title(main = "Random Forest Pooled ROC")
ct_sands <- performance(pred_ct_pooled, "sens", "spec")
ct_sens <- unlist(ct_sands@y.values)
ct_spec <- unlist(ct_sands@x.values)
ct_cutoffs <- unlist(ct_sands@alpha.values)
ct_J_scores <- ct_sens + ct_spec - 1
ct_best_index <- which.max(ct_J_scores)
ct_best_cutoff <- ct_cutoffs[ct_best_index]
print(paste("Best cutoff based on Youden's J: ", ct_best_cutoff))
accuracy <- (ct_sens + ct_spec) / 2
plot(ct_cutoffs, accuracy, type = "l", xlab = "Cutoff", ylab = "Accuracy", main = "Classification Trees Accuracy vs. Cutoff")
abline(v = ct_best_cutoff, col = "red", lty = 2)
ct_best_sens <- (unlist(perf_ct_pooled@y.values))[ct_best_index]
ct_best_spec <- (unlist(perf_ct_pooled@x.values))[ct_best_index]
plot(perf_ct_pooled, colorize = TRUE)
points(ct_best_spec, ct_best_sens, col="black", pch=16, cex=1.5)
text(ct_best_spec, ct_best_sens, labels="Youden's Index", pos=4, col="black")
title(main = "Classification Trees Pooled ROC")
ada_sands <- performance(pred_ada_pooled, "sens", "spec")
ada_sens <- unlist(ada_sands@y.values)
ada_spec <- unlist(ada_sands@x.values)
ada_cutoffs <- unlist(ada_sands@alpha.values)
ada_J_scores <- ada_sens + ada_spec - 1
ada_best_index <- which.max(ada_J_scores)
ada_best_cutoff <- ada_cutoffs[ada_best_index]
print(paste("Best cutoff based on Youden's J: ", ada_best_cutoff))
accuracy <- (ada_sens + ada_spec) / 2
plot(ada_cutoffs, accuracy, type = "l", xlab = "Cutoff", ylab = "Accuracy", main = "Adaptive Boosting Accuracy vs. Cutoff")
abline(v = ada_best_cutoff, col = "red", lty = 2)
ada_best_sens <- (unlist(perf_ada_pooled@y.values))[ada_best_index]
ada_best_spec <- (unlist(perf_ada_pooled@x.values))[ada_best_index]
plot(perf_ada_pooled, colorize = TRUE)
points(ada_best_spec, ada_best_sens, col="black", pch=16, cex=1.5)
text(ada_best_spec, ada_best_sens, labels="Youden's Index", pos=4, col="black")
title(main = "Adaptive Boosting Pooled ROC")
xgb_sands <- performance(pred_xgb_pooled, "sens", "spec")
xgb_sens <- unlist(xgb_sands@y.values)
xgb_spec <- unlist(xgb_sands@x.values)
xgb_cutoffs <- unlist(xgb_sands@alpha.values)
xgb_J_scores <- xgb_sens + xgb_spec - 1
xgb_best_index <- which.max(xgb_J_scores)
xgb_best_cutoff <- xgb_cutoffs[xgb_best_index]
print(paste("Best cutoff based on Youden's J: ", xgb_best_cutoff))
accuracy <- (xgb_sens + xgb_spec) / 2
plot(xgb_cutoffs, accuracy, type = "l", xlab = "Cutoff", ylab = "Accuracy", main = "eXtreme Gradient Boosting Accuracy vs. Cutoff")
abline(v = xgb_best_cutoff, col = "red", lty = 2)
xgb_best_sens <- (unlist(perf_xgb_pooled@y.values))[xgb_best_index]
xgb_best_spec <- (unlist(perf_xgb_pooled@x.values))[xgb_best_index]
plot(perf_xgb_pooled, colorize = TRUE)
points(xgb_best_spec, xgb_best_sens, col="black", pch=16, cex=1.5)
text(xgb_best_spec, xgb_best_sens, labels="Youden's Index", pos=4, col="black")
title(main = "eXtreme Gradient Boosting Pooled ROC")
xgb_ch <- performance(pred_xgb_pooled, "rch")
plot(xgb_ch, add = TRUE, lty = 2)
write.csv(final_performance_table, "ETTX final performance table with PA genes.csv")
write.csv(final_performance_table, "ETTX final performance table with PA genes.csv")
library(dplyr)
library(tidyr)
# Reshape from wide to long format
long_df <- final_performance_table %>%
pivot_longer(-Metric, names_to = "Model", values_to = "Value") %>%
mutate(Value = as.numeric(Value))  # Ensure numeric values for aggregation
# Calculate mean and SEM per metric per model
summary_table <- long_df %>%
group_by(Metric, Model) %>%
summarise(
Mean = mean(Value, na.rm = TRUE),
SEM = sd(Value, na.rm = TRUE) / sqrt(n()),
.groups = "drop"
)
# Optionally, format Mean ± SEM as a string
summary_table <- summary_table %>%
mutate(`Mean ± SEM` = sprintf("%.3f ± %.3f", Mean, SEM))
# View it
print(summary_table)
library(knitr)
library(kableExtra)
summary_table %>%
select(Metric, Model, `Mean ± SEM`) %>%
pivot_wider(names_from = Model, values_from = `Mean ± SEM`) %>%
kable("html", caption = "Performance Metrics Summary (Mean ± SEM)") %>%
kable_styling(full_width = FALSE)
# Define the models in the same order as your original color usage
model_order <- c("Naive.Bayes", "Logistic", "LASSO", "EN", "RF", "Class.Tree", "adaBoost", "XGBoost")
# Assign colors from the Paired palette in the correct order
colors <- brewer.pal(8, "Paired")
names(colors) <- model_order
# Filter and reorder AUC summary data
auc_summary <- summary_table %>%
filter(Metric == "AUC") %>%
mutate(Model = factor(Model, levels = model_order))
# Plot with consistent colors
ggplot(auc_summary, aes(x = Model, y = Mean, fill = Model)) +
geom_bar(stat = "identity", color = "black", width = 0.7) +
geom_errorbar(aes(ymin = Mean - SEM, ymax = Mean + SEM), width = 0.2) +
scale_fill_manual(values = colors) +
theme_minimal(base_size = 14) +
labs(
title = "AUC Comparison Across ML Models",
x = "Model",
y = "Mean AUC ± SEM"
) +
ylim(0, 1) +
theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")
document()
load_all()
table <- read.csv("final performance table corrected code.csv")
load_all()
table <- read.csv("final performance table corrected code.csv")
table <- read.csv("final performance table corrected code.csv")
View(table)
table <- read.csv("final performance table corrected code.csv", row.names = 1)
# Reshape from wide to long format
long_df <- table %>%
pivot_longer(-Metric, names_to = "Model", values_to = "Value") %>%
mutate(Value = as.numeric(Value))  # Ensure numeric values for aggregation
# Calculate mean and SEM per metric per model
summary_table <- long_df %>%
group_by(Metric, Model) %>%
summarise(
Mean = mean(Value, na.rm = TRUE),
SEM = sd(Value, na.rm = TRUE) / sqrt(n()),
.groups = "drop"
)
# Optionally, format Mean ± SEM as a string
summary_table <- summary_table %>%
mutate(`Mean ± SEM` = sprintf("%.3f ± %.3f", Mean, SEM))
# Optional: Make it a pretty table
library(knitr)
library(kableExtra)
summary_table %>%
select(Metric, Model, `Mean ± SEM`) %>%
pivot_wider(names_from = Model, values_from = `Mean ± SEM`) %>%
kable("html", caption = "Performance Metrics Summary (Mean ± SEM)") %>%
kable_styling(full_width = FALSE)
View(summary_table)
write.csv(summary_table, "ETTX nCV Metrics Summary Table.csv")
# Apply filtering and factor level ordering
filtered_summary <- summary_table %>%
filter(!Metric %in% c("TP", "TN")) %>%
mutate(Model = factor(Model, levels = model_order))
filtered_summary %>%
select(Metric, Model, `Mean ± SEM`) %>%
pivot_wider(names_from = Model, values_from = `Mean ± SEM`) %>%
kable("html", caption = "Performance Metrics Summary (Mean ± SEM)") %>%
kable_styling(full_width = FALSE)
# Apply filtering and factor level ordering
filtered_summary <- summary_table %>%
filter(!Metric %in% c("TP", "TN", "FP", "FN")) %>%
mutate(Model = factor(Model, levels = model_order))
filtered_summary %>%
select(Metric, Model, `Mean ± SEM`) %>%
pivot_wider(names_from = Model, values_from = `Mean ± SEM`) %>%
kable("html", caption = "Performance Metrics Summary (Mean ± SEM)") %>%
kable_styling(full_width = FALSE)
write.csv(filtered_summary, "Performance Metrics Summary.csv")
browseVignettes(package = "NestCV")
browseVignettes(package = NestCV)
browseVignettes(package = 'NestCV')
foo.Rmd --> foo.md --> foo.html
list.files()
rmarkdown::render("foo.R")
list.files()
devtools::load_all(".")
list.files()
document()
load_all()
document()
load_all()
document()
load_all()
load("~/NestCV/data/cran_packages.rda")
cran_packages
library(devtools)
devtools::install_github("drkgil/NestCV")
library(NestCV)
devtools::install_github("drkgil/NestCV")
devtools::install_github("drkgil/NestCV")
install.packages("RColorBrewer")
install.packages("RColorBrewer")
library(RColorBrewer)
package_version("RColorBrewer")
packageVersion("RColorBrewer")
packageVersion("dplyr")
packageVersion("psych")
packageVersion("reshape2")
packageVersion("shapviz")
devtools::install_github("drkgil/NestCV")
devtools::install_github("drkgil/NestCV")
devtools::install_github("drkgil/NestCV")
library(NestCV)
devtools::install_github("drkgil/NestCV")
library(NestCV)
NestCV::load_packages()
# these objects are defined in the package and can be used as an example!
prepare_patient_data(
data = ettx_Binary,
annotation = ettx_annotation,
gene_list = ETTX_genes
)
lapply(list.files("R", full.names = TRUE), source)
NestCV::prepare_patient_data(data = ettx_Binary, annotation = ettx_annotation, gene_list = ETTX_genes)
NestCV::prepare_patient_data(data = NestCV::ettx_Binary, annotation = NestCV::ettx_annotation, gene_list = NestCV::ETTX_genes)
source("R/load_packages.R")
devtools::document()
devtools::document()
devtools::load_all()
devtools::check()
update.packages(ask = FALSE, checkBuilt = TRUE)
update.packages(ask = FALSE, checkBuilt = TRUE)
library(devtools)
find_rtools()
has_devel()
